{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('deep': conda)",
   "metadata": {
    "interpreter": {
     "hash": "603f24775145c65333f1c742e61c601378f3aeb8f9c8cfe32a00b0ff9d9136cd"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Explorative search, graph-based discovery of text documents\n",
    "\n",
    "This notebook introduces a document-word graph-based search technique that leverages NLP and Pagerank, to allow you to explore text collections and discover unexpected relations between them. At the end, we get a way to retrieve documents related to a keyword, or that share words similar to other documents. \n",
    "\n",
    "### Introduction\n",
    "\n",
    "Imagine you have a collection of text documents, and you want to retrieve those which are related to a keyword. A rule of thumb would be : type a keyword, get me all the documents that have this keyword in their text. To rank those results by \"relevance\", classical techniques, such as [Okapi BM25](https://en.wikipedia.org/wiki/Okapi_BM25), are based around Term Frequency (how much the keyword is present in the document) and Inverse Document Frequency (how rare and specific the keyword is). Documents where the keyword is frequent are ranked higher than those where it's not.\n",
    "\n",
    "**The traditional techniques works well if you know exactly what you are looking for. But if you don't, they fall appart.**  \n",
    "\n",
    "This is especially the case when dealing with a large knowledge base, where you don't really know what's inside. Instead of looking up for a perticular document, you usually want to just explore the collection, discover interesting relations, or get a better undersanding of the knowledge base. For example, by searching for the keyword \"yoghourt\", you'd actually be interested in learning about \"milk industry\", \"agricultural sector\", \"rural areas\", and other stuff loosely related to yoghourt.\n",
    "\n",
    "The technique I introduce here helps you discover a text-based knowledge base by leveraging a document-word graph representation of the document collection. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Data \n",
    "\n",
    "We will be working with a dataset of ABC news headlines (Austalian Broadcasting Corporation). Thanks to Rohit Kulkarni for [sharing this dataset on Kaggle](https://www.kaggle.com/therohk/million-headlines). There are more than a million headlines in this dataset. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of documents: 1186018\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/abcnews-date-text.csv\")\n",
    "print(f\"Number of documents: {df.shape[0]}\")"
   ]
  },
  {
   "source": [
    "## Creating a graph out of text\n",
    "\n",
    "A bag-of-word embedding is a vector representation of text, where each row is a document and each column is a word. In its simplest form, if a word is in a document, then there is a 1 in the cell. Otherwise, there is 0. \n",
    "\n",
    "For a corpus of text, this representation can be interpretated as a bigraph adjacency matrix. That is, the mathematical encoding of a graph, where documents are linked to each other through shared words. This is called a document-word graph.\n",
    "\n",
    "Below, on the left, you have 30 random article headlines. On the right, they are connected to words. You can jump from an article to another by following their common words. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ],
      "image/svg+xml": "<svg height=\"340\" width=\"700.0\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M 32 320 632 175\" stroke=\"black\" stroke-width=\"1\"/><path d=\"M 32 309 632 175\" stroke=\"black\" stroke-width=\"1\"/><path d=\"M 32 288 632 164\" stroke=\"black\" stroke-width=\"1\"/><path d=\"M 32 299 632 164\" stroke=\"black\" stroke-width=\"1\"/><circle cx=\"32\" cy=\"320\" r=\"7.0\" style=\"fill:gray;stroke:black;stroke-width:1.0\"/><circle cx=\"32\" cy=\"154\" r=\"7.0\" style=\"fill:gray;stroke:black;stroke-width:1.0\"/><circle cx=\"32\" cy=\"40\" r=\"7.0\" style=\"fill:gray;stroke:black;stroke-width:1.0\"/><circle cx=\"32\" cy=\"51\" r=\"7.0\" style=\"fill:gray;stroke:black;stroke-width:1.0\"/><circle cx=\"32\" cy=\"61\" r=\"7.0\" style=\"fill:gray;stroke:black;stroke-width:1.0\"/><circle cx=\"32\" cy=\"309\" r=\"7.0\" style=\"fill:gray;stroke:black;stroke-width:1.0\"/><circle cx=\"32\" cy=\"71\" r=\"7.0\" style=\"fill:gray;stroke:black;stroke-width:1.0\"/><circle cx=\"32\" cy=\"82\" r=\"7.0\" style=\"fill:gray;stroke:black;stroke-width:1.0\"/><circle cx=\"32\" cy=\"92\" r=\"7.0\" style=\"fill:gray;stroke:black;stroke-width:1.0\"/><circle cx=\"32\" cy=\"102\" r=\"7.0\" style=\"fill:gray;stroke:black;stroke-width:1.0\"/><circle cx=\"32\" cy=\"113\" r=\"7.0\" style=\"fill:gray;stroke:black;stroke-width:1.0\"/><circle cx=\"32\" cy=\"123\" r=\"7.0\" style=\"fill:gray;stroke:black;stroke-width:1.0\"/><circle cx=\"32\" cy=\"133\" r=\"7.0\" style=\"fill:gray;stroke:black;stroke-width:1.0\"/><circle cx=\"32\" cy=\"144\" r=\"7.0\" style=\"fill:gray;stroke:black;stroke-width:1.0\"/><circle cx=\"32\" cy=\"30\" r=\"7.0\" style=\"fill:gray;stroke:black;stroke-width:1.0\"/><circle cx=\"32\" cy=\"288\" r=\"7.0\" style=\"fill:gray;stroke:black;stroke-width:1.0\"/><circle cx=\"32\" cy=\"20\" r=\"7.0\" style=\"fill:gray;stroke:black;stroke-width:1.0\"/><circle cx=\"32\" cy=\"164\" r=\"7.0\" style=\"fill:gray;stroke:black;stroke-width:1.0\"/><circle cx=\"32\" cy=\"175\" r=\"7.0\" style=\"fill:gray;stroke:black;stroke-width:1.0\"/><circle cx=\"32\" cy=\"185\" r=\"7.0\" style=\"fill:gray;stroke:black;stroke-width:1.0\"/><circle cx=\"32\" cy=\"195\" r=\"7.0\" style=\"fill:gray;stroke:black;stroke-width:1.0\"/><circle cx=\"32\" cy=\"206\" r=\"7.0\" style=\"fill:gray;stroke:black;stroke-width:1.0\"/><circle cx=\"32\" cy=\"216\" r=\"7.0\" style=\"fill:gray;stroke:black;stroke-width:1.0\"/><circle cx=\"32\" cy=\"226\" r=\"7.0\" style=\"fill:gray;stroke:black;stroke-width:1.0\"/><circle cx=\"32\" cy=\"237\" r=\"7.0\" style=\"fill:gray;stroke:black;stroke-width:1.0\"/><circle cx=\"32\" cy=\"247\" r=\"7.0\" style=\"fill:gray;stroke:black;stroke-width:1.0\"/><circle cx=\"32\" cy=\"257\" r=\"7.0\" style=\"fill:gray;stroke:black;stroke-width:1.0\"/><circle cx=\"32\" cy=\"299\" r=\"7.0\" style=\"fill:gray;stroke:black;stroke-width:1.0\"/><circle cx=\"32\" cy=\"268\" r=\"7.0\" style=\"fill:gray;stroke:black;stroke-width:1.0\"/><circle cx=\"32\" cy=\"278\" r=\"7.0\" style=\"fill:gray;stroke:black;stroke-width:1.0\"/><circle cx=\"632\" cy=\"164\" r=\"7.0\" style=\"fill:gray;stroke:black;stroke-width:1.0\"/><circle cx=\"632\" cy=\"175\" r=\"7.0\" style=\"fill:gray;stroke:black;stroke-width:1.0\"/><text font-size=\"12\" text-anchor=\"end\" x=\"22\" y=\"320\">0</text><text font-size=\"12\" text-anchor=\"end\" x=\"22\" y=\"154\">1</text><text font-size=\"12\" text-anchor=\"end\" x=\"22\" y=\"40\">2</text><text font-size=\"12\" text-anchor=\"end\" x=\"22\" y=\"51\">3</text><text font-size=\"12\" text-anchor=\"end\" x=\"22\" y=\"61\">4</text><text font-size=\"12\" text-anchor=\"end\" x=\"22\" y=\"309\">5</text><text font-size=\"12\" text-anchor=\"end\" x=\"22\" y=\"71\">6</text><text font-size=\"12\" text-anchor=\"end\" x=\"22\" y=\"82\">7</text><text font-size=\"12\" text-anchor=\"end\" x=\"22\" y=\"92\">8</text><text font-size=\"12\" text-anchor=\"end\" x=\"22\" y=\"102\">9</text><text font-size=\"12\" text-anchor=\"end\" x=\"22\" y=\"113\">10</text><text font-size=\"12\" text-anchor=\"end\" x=\"22\" y=\"123\">11</text><text font-size=\"12\" text-anchor=\"end\" x=\"22\" y=\"133\">12</text><text font-size=\"12\" text-anchor=\"end\" x=\"22\" y=\"144\">13</text><text font-size=\"12\" text-anchor=\"end\" x=\"22\" y=\"30\">14</text><text font-size=\"12\" text-anchor=\"end\" x=\"22\" y=\"288\">15</text><text font-size=\"12\" text-anchor=\"end\" x=\"22\" y=\"20\">16</text><text font-size=\"12\" text-anchor=\"end\" x=\"22\" y=\"164\">17</text><text font-size=\"12\" text-anchor=\"end\" x=\"22\" y=\"175\">18</text><text font-size=\"12\" text-anchor=\"end\" x=\"22\" y=\"185\">19</text><text font-size=\"12\" text-anchor=\"end\" x=\"22\" y=\"195\">20</text><text font-size=\"12\" text-anchor=\"end\" x=\"22\" y=\"206\">21</text><text font-size=\"12\" text-anchor=\"end\" x=\"22\" y=\"216\">22</text><text font-size=\"12\" text-anchor=\"end\" x=\"22\" y=\"226\">23</text><text font-size=\"12\" text-anchor=\"end\" x=\"22\" y=\"237\">24</text><text font-size=\"12\" text-anchor=\"end\" x=\"22\" y=\"247\">25</text><text font-size=\"12\" text-anchor=\"end\" x=\"22\" y=\"257\">26</text><text font-size=\"12\" text-anchor=\"end\" x=\"22\" y=\"299\">27</text><text font-size=\"12\" text-anchor=\"end\" x=\"22\" y=\"268\">28</text><text font-size=\"12\" text-anchor=\"end\" x=\"22\" y=\"278\">29</text><text font-size=\"12\" x=\"642\" y=\"164\">panthers</text><text font-size=\"12\" x=\"642\" y=\"175\">stage</text></svg>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sknetwork.visualization import svg_bigraph\n",
    "from IPython.display import SVG\n",
    "\n",
    "nb_sub_samples = 30\n",
    "stop_words = [\"I\", \"the\", \"he\", \"she\", \"it\", \"is\", \"of\", \"to\", \"in\", \"his\", \"her\", \"their\", \"its\", \"a\", \"out\", \"of\", \"on\", \"up\", \"with\", \"for\", \"at\", \"should\", \"not\", \"is\", \"as\", \"no\"]\n",
    "small_df = df.sample(nb_sub_samples)\n",
    "\n",
    "count_2 = CountVectorizer(min_df=2, binary=True, stop_words=stop_words)\n",
    "small_adjacency = count_2.fit_transform(small_df[\"headline_text\"])\n",
    "reverse_vocab = {v: k for k, v in count_2.vocabulary_.items()}\n",
    "names_col = [reverse_vocab[i] for i in range(len(reverse_vocab))]\n",
    "names_row = list(range(nb_sub_samples))\n",
    "\n",
    "image = svg_bigraph(small_adjacency, names_row=names_row, names_col=names_col, width=600)\n",
    "SVG(image)"
   ]
  },
  {
   "source": [
    "Let's now do this on the full dataset. Since it's so big, we won't be showing the graph. Note that this graph only needs to be computed once. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of the full adjacency matrix: (1186018, 62187)\nNumber of non-zero elements: 6670374\nCPU times: user 13.3 s, sys: 262 ms, total: 13.6 s\nWall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "count = CountVectorizer(min_df=2, binary=True, stop_words=stop_words)\n",
    "adjacency = count.fit_transform(df['headline_text'])\n",
    "print(\"Shape of the full adjacency matrix:\", adjacency.shape)\n",
    "print(\"Number of non-zero elements:\", adjacency.nnz)"
   ]
  },
  {
   "source": [
    "## Using PageRank to search through documents\n",
    "\n",
    "We are going to use a variant of PageRank algorithm on this adjacency matrix. \n",
    "\n",
    "### The PageRank algorithm\n",
    "\n",
    "PageRank is an algorithm developed by Google in 2000s to find relevant websites based on their URL links. At the time, websites at the top of Google seach were those who were the most linked to, and by websites that were also the most linked to. This ranking of websites is made by averaging several random walks on the graph of website to website links. \n",
    "\n",
    "Imagine robots following randomly links on webpages, and keeping a record on which websites they end up most often : those would be at the top of the search results. Markov chain theory guarantees us that this algorithm converges towards a unique ranking. \n",
    "\n",
    "BiPageRank is the same idea, but applied to bigraphs. The algorithm performs a random walk from an article, to a word, to another article, to a word, and so on. \n",
    "\n",
    "Computationally, this random walk is nothing more than matrix multiplications. \n",
    "\n",
    "### Personalized PageRank\n",
    "\n",
    "The variant we will be using is called Personalized BiPageRank. Basically, instead of starting our random walk anywhere on the graph, we make it start on a specific node of the graph. To get search results relevant to a word, we will be starting our random walk on this word's node. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1: tsunami threat overhyped\n2: tunbridge dam leak threat eases\n3: mccrae residents face fire threat\n4: mccrae grassfire no longer a threat\n5: sandery death threat misunderstanding killa nickname\n6: patient blacklisting threat symptomatic of\n7: wanilla fire threat eases\n8: kinkuna homes threat of bushfire\n9: pichi richi marathon under threat\n10: jo goodhew milk threat\n"
     ]
    }
   ],
   "source": [
    "from sknetwork.ranking import BiPageRank, top_k\n",
    "\n",
    "bpr = BiPageRank()\n",
    "scores = bpr.fit_transform(adjacency, seeds_col={count.vocabulary_[\"threat\"]:1})\n",
    "\n",
    "for rank, (i, row) in enumerate(df.iloc[top_k(scores, 10)].iterrows()):\n",
    "    print(f\"{rank+1}: {row['headline_text']}\")"
   ]
  },
  {
   "source": [
    "We can also do it by starting on an article's node. This acts as a recommendation of \"similar articles\". The article we look at is \"more women urged to become councillors\". It yields us articles about various urges over councillors."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 (idx: 100) : more women urged to become councillors\n2 (idx: 182223) : push on for more women councillors\n3 (idx: 883878) : councillors urged to back more wholistic way to\n4 (idx: 107383) : blokes chauvinism dudding potential councillors\n5 (idx: 372296) : new councillors urged to quickly grasp issues\n6 (idx: 876874) : councillors urged to back old castlemaine gaol\n7 (idx: 659286) : lake councillors urged to reject biodiversity offsets policy\n8 (idx: 450766) : lake wooloweyah subdivision divides councillors\n9 (idx: 1026213) : perth councillors urged not to criticise lord mayor scaffidi\n10 (idx: 43461) : councillors urged to consider roles during\n"
     ]
    }
   ],
   "source": [
    "scores = bpr.fit_transform(adjacency, seeds_row={100:1})\n",
    "\n",
    "for rank, (i, row) in enumerate(df.iloc[top_k(scores, 10)].iterrows()):\n",
    "    print(f\"{rank+1} (idx: {i}) : {row['headline_text']}\")"
   ]
  },
  {
   "source": [
    "We can also start from several nodes, both articles and words, to perform complex searches. For example, let's look at articles similar to \"more women urged to become councillors\", but also to \"police\" and \"fire\". The third result is about a man charged (i.e. arrested by the *police*) after setting a *fire* in *councillors* office : very relevant to our bizarre search query. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 (idx: 100) : more women urged to become councillors\n2 (idx: 189645) : councillors under fire after pay rise\n3 (idx: 767815) : man charged over brisbane councillors office fire\n4 (idx: 126586) : councillors under fire over adult entertainment\n5 (idx: 125656) : councillors want no fire ban exemptions\n6 (idx: 636501) : no show councillors under fire\n7 (idx: 182223) : push on for more women councillors\n8 (idx: 883878) : councillors urged to back more wholistic way to\n9 (idx: 279629) : police meet councillors over noble park hoons\n10 (idx: 979367) : bendigo councillors escorted by police\n11 (idx: 958551) : dungog councillors urged to act on storm damage in aged care un\n12 (idx: 372296) : new councillors urged to quickly grasp issues\n13 (idx: 876874) : councillors urged to back old castlemaine gaol\n14 (idx: 107383) : blokes chauvinism dudding potential councillors\n15 (idx: 940414) : councillors urged to approve bendigo hotel pokies\n16 (idx: 1026213) : perth councillors urged not to criticise lord mayor scaffidi\n17 (idx: 659286) : lake councillors urged to reject biodiversity offsets policy\n18 (idx: 299223) : councillors urged to back nightclubs crackdown\n19 (idx: 875792) : councillors urged to reject pretty pool fifo camp\n20 (idx: 43461) : councillors urged to consider roles during\nCPU times: user 6.56 s, sys: 1.39 s, total: 7.96 s\nWall time: 8.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores = bpr.fit_transform(adjacency, seeds_row={100:1}, seeds_col={count.vocabulary_[\"police\"]:1, count.vocabulary_[\"fire\"]:1})\n",
    "\n",
    "for rank, (i, row) in enumerate(df.iloc[top_k(scores, 20)].iterrows()):\n",
    "    print(f\"{rank+1} (idx: {i}) : {row['headline_text']}\")"
   ]
  },
  {
   "source": [
    "## Tinkering with the graph\n",
    "\n",
    "Quality and relevancy of search results heavily depends on the quality of the underlying graph. \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Lower the word quantity\n",
    "\n",
    "According to my experience, this search technique works best with short documents. The idea is that signal-to-noise ratio is high: with too many parasit words, you'll have less relevant results. But also, the algorithm is much faster with a sparse and small graph. In that sense, rare and meaningful links are great. \n",
    "\n",
    "#### Stemming and lemmatization\n",
    "\n",
    "These are two NLP techniques that clean up the text before building the graph. \n",
    "- **[Stemming](https://www.geeksforgeeks.org/python-stemming-words-with-nltk/)** : remove the varying end of words. Ex: liking, likely, liked, likes => like \n",
    "- **[Lemmatization](https://www.geeksforgeeks.org/python-lemmatization-with-nltk/)** : get back to the root of the word. Ex: better => good, rocks => rock, maxima => maximum\n",
    "\n",
    "This way, singular and plurals become the same tokens (words).\n",
    "\n",
    "Even though you only need to do it once for each document, lemmatization is costly. It needs to understand the role of each word in a sentence (noun, verb, adjective, etc.). In this notebook, we will perform it only on a susbsample of 30,000 documents. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 1min 25s, sys: 355 ms, total: 1min 25s\nWall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import spacy\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Lemmatization takes a long time, because it needs to guess the role of each word in a sentence\n",
    "# We will do it on a subsample of our dataset\n",
    "\n",
    "df_small = df.sample(30000)\n",
    "\n",
    "def lemmatize(sentence):\n",
    "    # Apply lemmatization to each word in a sentence\n",
    "    return [token.lemma_ for token in nlp(sentence)]\n",
    "\n",
    "df_small['cleaned_text'] = df_small[\"headline_text\"].map(lambda x: lemmatize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We already tokenized our text in the previous step, so we change our preprocessor and tokenizer to a dummy function\n",
    "def dummy(x):\n",
    "    return x\n",
    "\n",
    "count_lem = CountVectorizer(min_df=2, binary=True, stop_words=stop_words, preprocessor=dummy, tokenizer=dummy, token_pattern=None)\n",
    "adjacency_small_lem = count_lem.fit_transform(df_small[\"cleaned_text\"].to_list())\n",
    "\n",
    "# We will also compare the results to the method without lemmatization\n",
    "count_small = CountVectorizer(min_df=2, binary=True, stop_words=stop_words)\n",
    "adjacency_small = count_small.fit_transform(df_small[\"headline_text\"])"
   ]
  },
  {
   "source": [
    "With lemmatization, we have ~20% less tokens. Since PageRank works by doing matrix multiplication, at scale, this could be a huge improvement in inference time. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Without lemmatization: 11691 tokens.\nWith lemmatization: 9393 tokens.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Without lemmatization: {len(count_small.vocabulary_)} tokens.\")\n",
    "print(f\"With lemmatization: {len(count_lem.vocabulary_)} tokens.\")"
   ]
  },
  {
   "source": [
    "We run the algorithm on a query example : articles similar to \"olympic swimmer sweating on fight probe\". \n",
    "\n",
    "Without lemmatization, the explorative search algorithm ranks higher articles with the exact word \"sweating\", such as \"johnson clark still sweating on decision\". "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 (idx: 373411) : olympic swimmer sweating on fight probe\n2 (idx: 269270) : johnson clark still sweating on decision\n3 (idx: 832030) : indefinite suspension for olympic champion swimmer sun yang\n4 (idx: 887430) : swimmer allegedly abused by volkers insulted by prosecutor\n5 (idx: 888323) : swimmer heads to glasgow inspired by olympics great aunt\n6 (idx: 1056047) : 91yo swimmer ready for masters games gold coast\n7 (idx: 924912) : police concerned for missing swimmer murrumbidgee river\n8 (idx: 1068448) : former gold medal swimmer bec creedy eyes ironwoman crown\n9 (idx: 427713) : police divers to join search for missing swimmer\n10 (idx: 505798) : swimmer molested by leading coach\nCPU times: user 141 ms, sys: 0 ns, total: 141 ms\nWall time: 162 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Without lemmatization\n",
    "scores = bpr.fit_transform(adjacency_small, seeds_row={1000:1})\n",
    "\n",
    "for rank, (i, row) in enumerate(df_small.iloc[top_k(scores, 10)].iterrows()):\n",
    "    print(f\"{rank+1} (idx: {i}) : {row['headline_text']}\")"
   ]
  },
  {
   "source": [
    "With lemmatization, the explorative search algorithm ranks higher articles which have any form of the verb \"sweat\" : sweating, sweat, sweats... This is because all of these form are, in lemmatized form, the same token. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 (idx: 373411) : olympic swimmer sweating on fight probe\n2 (idx: 821590) : roosters sweat on johnston for sanfl prelim\n3 (idx: 269270) : johnson clark still sweating on decision\n4 (idx: 1156803) : timber industry sweats on future\n5 (idx: 815830) : murray turns up heat as us open sweats\n6 (idx: 832030) : indefinite suspension for olympic champion swimmer sun yang\n7 (idx: 888323) : swimmer heads to glasgow inspired by olympics great aunt\n8 (idx: 681462) : more aussie swimmers bound for olympics\n9 (idx: 566963) : disabled swimmers deserve bigger presence cowdrey\n10 (idx: 887430) : swimmer allegedly abused by volkers insulted by prosecutor\nCPU times: user 140 ms, sys: 149 µs, total: 140 ms\nWall time: 177 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# With lemmatization\n",
    "scores = bpr.fit_transform(adjacency_small_lem, seeds_row={1000:1})\n",
    "\n",
    "for rank, (i, row) in enumerate(df_small.iloc[top_k(scores, 10)].iterrows()):\n",
    "    print(f\"{rank+1} (idx: {i}) : {row['headline_text']}\")"
   ]
  },
  {
   "source": [
    "#### Keywords summary\n",
    "\n",
    "If you have long documents, summing them up to their keywords will allow you to efficiently shorten them. There are different techniques to do so. To stay in theme, I'll show you two graph-based ones. \n",
    "\n",
    "1. co occurence matrix : document by document.\n",
    "\n",
    "https://cran.r-project.org/web/packages/textrank/vignettes/textrank.html\n",
    "\n",
    "2. co occurence matrix : whole dataset. Keep top 1000 keywords for example. Summarize each doc by the less frequent keyword first.\n",
    "\n",
    "co occurrence matrix + pagerank -> keyword list. Use as vocab\n",
    "\n",
    "```\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "docs = ['this this this book',\n",
    "        'this cat good',\n",
    "        'cat good shit']\n",
    "count_model = CountVectorizer(ngram_range=(1,1)) # default unigram model\n",
    "X = count_model.fit_transform(docs)\n",
    "# X[X > 0] = 1 # run this line if you don't want extra within-text cooccurence (see below)\n",
    "Xc = (X.T * X) # this is co-occurrence matrix in sparse csr format\n",
    "Xc.setdiag(0) # sometimes you want to fill same word cooccurence to 0\n",
    "print(Xc.todense()) # print out matrix in dense format\n",
    "```\n",
    "\n",
    "Other method :\n",
    "\n",
    "https://pypi.org/project/rake-nltk/\n",
    "\n",
    "Note that stop words are also super useful. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Vary words' importance\n",
    "\n",
    "Not all links matter the same. In the same spirit as keywords, you want to lower impact of common words, and increase impact of rare words.\n",
    "\n",
    "This is where IDF (Inverse Document Frequency) can come useful. Note that IDF was used in the traditional search algorithm [Okapi BM25](https://en.wikipedia.org/wiki/Okapi_BM25). We're getting the best of both worlds. \n",
    "\n",
    "todo: TF-IDF + threshold"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Using other metadata to link documents\n",
    "\n",
    "We are building a graph linking document to each other. But links don't have to only be words. Typically, you'd want to use metadata : tags, categories, dates, shared followers or subscribers... \n",
    "\n",
    "TODO : link documents by month+year\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### References\n",
    "\n",
    "- [Using Combined Document-Word Graphsfor Wikipedia Search](https://west.uni-koblenz.de/assets/theses/bachelorsthesis_elias_zervudakis_and_felix_engelmann.pdf), bachelor's thesis by Felix Engelmann and Elias Zervudakis. Same idea as this notebook, but they apply it on Wikipedia articles and evaluate search relevancy with user feedback. \n",
    "\n",
    "### Special thanks \n",
    "\n",
    "Thanks to Thomas Bonald, my teacher at Télécom Paris, for developing the amazing [scikit-network library](https://scikit-network.readthedocs.io/en/latest/index.html) to handle graphs and for opening me to the world of graphs.\n",
    "\n",
    "Thanks to [Foundamental](https://foundamental.com/), the company where I did an internship, and gave me the opportunity to experiment these ideas freely and giving me feedback on prototypes. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}